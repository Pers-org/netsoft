# Создание микросервиса

## 1. Работа с внешним API

Библиотека **requests** в Python позволяет выполнять HTTP запросы (GET, POST, PATCH, PUT, DELETE) к внешним API. Существует специальные фейковые API для тестирования, например, JSONPlaceholder. Восполльзуемся таким API для создания простого клиента.

Библиотека requests предлагает удобный интерфейс для выполнения HTTP запросов. В интерфейс входят функции get(), post(), patch(), put(), delete(). Рассмотрим примеры их использования.

```
import requests
import json 

# Демонстрация взаимодействия с API по HTTP

URL = "https://jsonplaceholder.typicode.com"

# GET запрос на получение всех записей
all_notes = requests.get(URL+"/posts")
all_notes = all_notes.json()

print("All notes:", all_notes)

# GET запрос на получение одной записи

note = requests.get(URL+"/posts/10")
note = note.json()

print("Single note:", note)

# POST запрос на создание записи

user = {
    "userId": 1362,
    "id": 1363,
    "title": "Bla-bla-bla",
    "body": "Bla-bla-bla"
}

response = requests.post(URL+"/posts", json=user)

print("Response status:", response)

```
Функция get() принимает URL адрес и возвращает объект response, который имеет следующие поля:

[![image.png](https://i.postimg.cc/NFcRvQSZ/image.png)](https://postimg.cc/0MVzp18C)

Используя метод json(), можно извлечь данные в json формате и вывести через print().

Метод post работает по такому же принципу, но дополнительно принимает данные, которые нужно отправить.

## 2. Создание своего REST API

### 2.1 О REST

REST (от англ. Representational State Transfer — «передача репрезентативного состояния» или «передача состояния представления») — архитектурный стиль взаимодействия компонентов распределённого приложения в сети. Другими словами, REST — это набор правил того, как программисту организовать написание кода серверного приложения, чтобы все системы легко обменивались данными и приложение можно было масштабировать.

![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQKTH_fQcCoLmyFHDZ7CmYqcRGWocgnFObTWQ&s)

Принципы:

* Ресурсно-ориентированность: Все данные рассматриваются как ресурсы, идентифицируемые уникальными URL.
* Использование HTTP-методов: Действия с ресурсами выполняются через стандартные методы HTTP:
GET — получение ресурса.
POST — создание нового ресурса.
PUT — полное обновление ресурса.
PATCH — частичное обновление.
DELETE — удаление.

* Отсутствие состояния: Сервер не хранит информацию о предыдущих запросах клиента; каждый запрос содержит все данные, необходимые для его выполнения.
* Единообразие интерфейса: Используется унифицированный подход к обращению к ресурсам.
* Клиент-серверная модель: Четкое разделение обязанностей.

Преимущества:

* Производительность
* Масштабируемость

### 2.2 Инструменты

Для создания REST API использовались следующие библиотеки:

* Fast API - современный, высокопроизводительный веб-фреймворк на Python для создания API, основанный на аннотациях типов Pydantic. Имеет удобный, понятный интерфейс и позволяет создавать API буквально в пару строк.

* Pydantic - библиотека для Python, предназначенная для валидации (проверки), сериализации и трансформации данных с использованием аннотаций типов. Нужна для сериализации данных, приходящих на сервер.

* Pytest - популярный, мощный и простой в использовании фреймворк для тестирования кода на Python. Подходит для тестирования API, созданных с помощью библиотеки Fast API.

### 2.3 Создание API

Необходимые импорты:

```
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
```

Создадим ресурс, с которым будет работать API. В моем случае это объект, описывающий книгу:

```
# Create Book class with pydantic
class Book(BaseModel):
    id: int | None = None
    title: str
    author: str
    description: str | None = None
    price: float
```

Здесь строка id: int | None = None означает, что id либо имеет тип int либо тип None, а по дефолту имеет тип None

Для создания endpoints имеется удобная конструкция на основе декоратора, которая выглядит следующим образом:

```
# GET endpoint. Return all books from db
@app.get("/books")
def get_books():
    return book_db
```

Здесь от нас требуется написать функцию обработки endpoint (всё что после def). Endpoint просто возвращает спсиок всех книг. Далее наша функция оборачивается в декоратор, который автоматически добавляет дополнительный функционал в нашу функцию. Доступ к endpoint можно получить по адресу localhost:port/books.

Остальные endpoints создаются по такому же принципу, меняется только название декоратора (@app.put, @app.delete и т.д).

#### Удобный функционал:

Есть возможность через адресную строку браузера передавать дополнительную информацию (например, id) и удобно принимать ее на сервере:

```
# GET endpoint. Return book with received id
@app.get("/books/{id}")
def get_books(id: int):
    # Your code
```

Теперь можно выполнять поиск конкретной книги по ID. Запрос может выглядеть так:

localhost:port/books/1

В случае POST нам нужно передавать структуру на сервер. Организоавать сериализацию и прием данных можно так:
```
# POST endpoint. Write new book to db
@app.post("/books")
def post_book(book: Book):
    # Your code
```

По большому счету от нас требуется только сказать, что endpoint ожидает данные типа Book, а все остальное делается под капотом.

## 3. Написание тестов

Тест пишется в отдельном файле. Желательно, чтобы файл находился в директории /tests. Библиотека pytest требует библиотеку httpx, поэтому ее нужно дополнительно установить.

Импорты:

```
import pytest
from fastapi.testclient import TestClient
from app.API import app, book_db
```

Необходимо сделать импорт объекта FastAPI из кода приложения, чтобы установить с ним соединение и проводить тесты. Соединение устанавливается следующим образом:

```
client = TestClient(app)
```

Мы создали объект клиент, который соединен с нашим API и может взаимодействовать с ним.

Также можно импортировать базу данных, чтобы очищать ее после каждого теста. Делается это с помощью встроенного декоратора:

```
# clear db before any test
@pytest.fixture(autouse=True)
def clear_db():
    book_db.clear()
    yield
```

Все тесты пишутся единообразно, например:

```
# TEST GET (for all books)
def test_get_all_books_empty():
    response = client.get("/books")
    assert response.status_code == 200
    assert response.json() == []
```

Здесь просто создаем функцию, в которой описываем логику теста. С помощью объекта client делаем запрос с API, а с помощью assert делаем проверки, которые нам нужны, например, проверяем код ответа и данные.

Запустить тест можно с помощью команды 

```
python -m pytest -q
```

команду нужно выполнять в корне проекта.

## 4. Работа с Docker

### 4.1 Про Docker

Открытая платформа для автоматизации развертывания приложений в легких изолированных средах, называемых **контейнерами**. 

Для развертывания приложений я буду использовать docker, поскольку он позволяет автоматизировать процесс развертывания, а также исключить проблемы с версиями приложений, зависимостями и т.д, благодаря изолированности пространства.

Докер чем-то схож с виртуальными машинами, но он намного лучше. Он выигрывает в удобстве использования и производительности, поскольку он работает поверх ОС и всего лишь изолирует пространство, в то время как ВМ работают поверх железа и требует отдельных ядер, памяти и т.д.  

![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTdj8-U5Gpv-cy-ca57IzabOAeVzuoAwPWuOA&s)

Докер состоит из трех составных частей:
- **Клиент (frontend)** - удобный интерфейс для работы с докером. Он предоставляет набор команд, но сам их исполнением не занимается, а передает их на backend 
- **Демон (backend)** - внутренняя часть докера. Исполняет команды
- **DockerHub** - хранилище контейнеров. 
Контейнеры очень легко импортировать и экспортировать, поэтому появилась необходимость в том, чтобы где-то хранить образы, делиться ими, заимствовать и т.д 


![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRkxjXDT-39UlCW0nmT-Wd4XwvqaYDgQCsKHw&s)

**Образ** - это как-бы чертеж для контейнера. У нас есть программа, с помощью файла Dockerfile мы собираем образ, а на основе образа (чертежа) мы можем запускать сколько угодно контейнеров.

**Контейнер**- если образ это чертеж, то контейнер - изделие, сделанное по чертежу. Это изолированное пространство, в котором выполняется какой то код.

**Полезные команды при работе с Docker:**
```
docker ps # показать запущенные контейнеры
docker ps -a # показать все контейнеры (даже удаленные)
docker run <image_name> # запустить контейнер
docker run -d IMAGE # в фоне
docker stop <container_name> # остановить контейнер
docker start <container_name> # запустить
docker restart <container_name> # перезапустить
docker rm <container_name> # удалить контейнер

docker images # показать образы
docker pull <image_name> # скачать образ
docker build -t name <path_to_file> # собрать образ из Dockerfile
docker rmi <image_name> # удалить образ
docker image prune # удалить неиспользуемые образы


docker exec -it <container_name> sh # зайти в контейнер
docker logs <container_name> # посмотреть логи
```

**Docker compose**

Это инструмент, который позволяет собирать многоконтейнерные приложения. Допустим, если поднимать облачное хранилище, то нам скорее всего нужен будет образ самого облачного хранилища (его логика, веб морда и т.д), база данных, кэш (redis, memcached) и т.д. 

Скачивать по отдельности каждый образ и по отдельности запускать как то неудобно, поэтому сразу несколько контейнеров можно описать и сразу настроить в специальном файле docker-compose.yaml и при выполнении этого файла соберется приложение из нескольких контейнеров.

**Полезные команды:**

 ```
 docker compose up # запуск приложения
 docker compose down # остановка приложения
 docker compose restart # перезапуск приложения

docker compose <имя_отдельного_приложения> # запустить отдельное приложение (допустим нужно поднять только БД)
 ```

### 4.2 Написание dockerfile

Для создание образа нужно выполнить последовательность действий, которые можно написать в специальном скрипте Dockerfile и передать в docker, который сам всё сделает. От нас требуется только написать в Dockerfile то, какой контейнер мы хотим, а всё остальное сделает сам docker.

Пример Dockerfile:

```
FROM ubuntu:22.04 AS builder
ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install -y \
    python3.11 python3-pip \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN python3.11 -m pip wheel -r requirements.txt -w /wheels


FROM python:3.11-slim

COPY requirements.txt .
COPY --from=builder /wheels /wheels

RUN pip install --no-index --find-links=/wheels -r requirements.txt

COPY ./app/API.py .

EXPOSE 8000
CMD ["python", "-m", "uvicorn", "API:app", "--host", "0.0.0.0", "--port", "8000"]

```

* FROM "image" - позволяет загрузить из DockerHub образ-основу. То есть можно взять почти готовый докер образ, установить доп.зависмости и получить нужный образ.
* workdir "dir" - задает директорию контейнера, для которой будут применяться все следующие действия.
* COPY "host_resource" "container_resource" - копирует файлы с хоста в контейнер.
* RUN "commands" - выполяет команды в контейнере.

### 4.3 multi-stage

Это подход в написании Dockerfile, позволяющий ускорить сборку образа и уменьшить его размер за счет разбиения сборки на отдельные задачи. Например, в моем Dockerfile 2 шага:

Шаг первый:

```
FROM ubuntu:22.04 AS builder

ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install -y \
    python3.11 python3-pip

COPY requirements.txt .

RUN python3.11 -m pip wheel -r requirements.txt -w /wheels
```

На этом шаге берем Ubuntu за основу, устанавливаем системные зависимости, копируем файл с зависимостями python и собираем зависимости в .whl файл БЕЗ установки (только загрузка).

Далее идет второй шаг:

```
FROM python:3.11-slim

COPY requirements.txt .
COPY --from=builder /wheels /wheels

RUN pip install --no-index --find-links=/wheels -r requirements.txt

COPY ./app/API.py .

EXPOSE 8000
CMD ["python", "-m", "uvicorn", "API:app", "--host", "0.0.0.0", "--port", "8000"]
```

Здесь из прошлого шага копируем .whl файлы и уже именно устананавливаем уже скаченные зависимости. Далее копируем нашу программу в контейнер.

В чем здесь выигрыш?

Выигрыш заключается в том, что теперь у нас разделены сборка зависимостей и их установка. В файлах .whl содержится только самое нужное, поэтому вес образа становится меньше. Например, для сборки зависимостей можно использовать Ubuntu (которая весит много, но в которой больше инструментов), а для самого контейнера использовать какой-нибудь очень легкий, урезанный образ, в котором даже пакетного менеджера нет.

Мой образ собрался за 76 секунд и весит 156Мб, в то время как образ на чистой ubuntu собрался за 62 секунды и весил 462Мб.

### 4.4 .dockerignore

.dockerignore файл это аналог .gitignore файла, но только для docker. Если .gitignore предотвращает добавление файлов в коммиты, то .dockerignore предотвращает передачу файлов docker daemon. Дело в том, что docker будет передавать ВЕСЬ контекст демону, если нет .dockerignore. Если контекстом является корень приложения, то демону будет передана куча ненужных ему файлов. 

Пример .dockerignore:

```
/.github
/.git
/.gitignore
/.pytest_cache
/examples
/helm
/tests
/venv
```

Я исключил все директории, которые не нужны демону.

Запуск сборки без .dockerignore

[![image.png](https://i.postimg.cc/qvpXRf4S/image.png)](https://postimg.cc/tYmVSM1N)

Видим, что в процессе сборки 3 раза файлы передаются демону:

1. Передается сам Dockerfile
2. Передается .dockerignore
3. Передается контекст 

Теперь добавим .dockerignore

[![image.png](https://i.postimg.cc/zXy3F0ph/image.png)](https://postimg.cc/nCtHHKyV)

Видим, что теперь копирование контекста составило всего 605 байт, а не 52.8мб, что в 715 раз меньше. Наглядно видно, как .dockerignore ускоряет процесс сборки.

### 4.5 requirements.txt

requirements.txt - файл, в котором описываются зависимости, которые необходимо установить.

Преимущество использования:

1. Удобство: все в одном файле.
2. Возможность задать версию (более правильный подход).
3. Ускорение сборки за счет того, что можно менять зависимости, но при этом не менять сам Dockerfile (так лучше для кэша).

Пример requirements.txt

```
fastapi==0.129.0
uvicorn==0.40.0
pydantic==2.11
pytest==9.0.2
httpx==0.27.0
```

### 4.5 docker-compose

Это инструмент, который позволяет собирать многоконтейнерные приложения. Допустим, если поднимать облачное хранилище, то нам скорее всего нужен будет образ самого облачного хранилища (его логика, веб морда и т.д), база данных, кэш (redis, memcached) и т.д. 

Скачивать по отдельности каждый образ и по отдельности запускать как то неудобно, поэтому сразу несколько контейнеров можно описать и сразу настроить в специальном файле docker-compose.yaml и при выполнении этого файла соберется приложение из нескольких контейнеров.

**Полезные команды:**

 ```
docker compose up # запуск приложения
docker compose down # остановка приложения
docker compose restart # перезапуск приложения

docker compose <имя_отдельного_приложения> # запустить отдельное приложение (допустим нужно поднять только БД)
 ```

Пример docker-compose:

```
version: "3.9"

services:
  db:
    image: postgres:alpine
    environment:
      POSTGRES_USER: postgress_user
      POSTGRES_PASSWORD: postgress_password
      POSTGRES_DB: books
    ports:
      - "5433:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data

  app:
    build: .
    depends_on:
      - db
    environment:
      DB_HOST: db_host
    ports:
      - "8000:8000"

volumes:
  pgdata:
```





